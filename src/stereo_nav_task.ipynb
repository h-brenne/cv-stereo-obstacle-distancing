{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When navigating with a robot, obstacle detection and avoidance is an important task. There are many sensors and solutions to get information about obstacles in the environment. One solution is using a stereo camera setup. \n",
    "\n",
    "If we match a 3D point to the corresponding 2D pixel coordinate on each camera, we can measure the horizontal disparity distance d between these pixel coordinates. With a higher disparity, the normal distance z from the cameras to the 3D point will be smaller. We can accurately calculate this distance with known camera and stereo setup parameters. We can calculate a disparity map by calculating the disparity between each pixel in one camera, and the corresponding pixel in the other. Finally, a simple measure of obstacles is to calculate a single main disparity, giving a single obstacle distance measurement in front of the robot. Using this measurement, one could create a simple reactive robot which for example turns around when sufficiently close to an obstacle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by calculating the disparity matrix. A huge computational advantage is to work with rectified images. We can then be sure that each horizontal row is colinear with the x-axis. Asuming no vertical shift between the two cameras, we can be sure that the corresponding point to match lies in the same horizontal pixel row on both cameras. If we only look for obstacles within a specific range, we can ... the search further. In addition, we will only calculate the disparity matrix in a square area in the center if the image, as we are interested in the obstacle directly in front of the robot.\n",
    "\n",
    "Simply comparing a single intensity value against others does not work well, as there isn't enough information to differentiate the pixels. We will compare a block of intenseties around the reference pixel with block of intensities around the candidate pixels in the other image. \n",
    "We need a measure of simmilarity between the blocks. In this project Sum of Absolute Difference(SAD) is used, but there are other measures as well. With SAD, we simply sum the absolute difference between the block around the reference pixel in one image, with the block around the candidate pixel. We then get a scalar value of the cost of choosing this candidate pixel. We want to find the candidate pixel with the lowest cost.\n",
    "\n",
    "Disparity has to be efficiently calculated, as this should run real time at a sufficient frame rate. With python, we definetly don't want to traverse over the image pixel by pixel, as even just accesing the data would be slow compared to more efficient solution. This problem can be efficiently solved with a box filter. When moving the block of the reference or candidate just one column or row, we simply need to add the new row or column, and remove the previous row or column. OpenCV filter2D has a efficient implementation that exploits this.\n",
    "\n",
    "We first crop the reference left image. Then for each disparity d, we crop the right image after shifting it d pixels using a transfromation matrix. The absoulte difference matrix is calculated from the cropped images. We use filter2D on this matrix with the sum kernel of just 1's, obtaining the SAD. The result is stored in the 3D \"volume\" C=(x,y,d). After all disparities have been calculated, we find the disparity matrix as the argmin of the axis d at every (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disparity_matrix(imgL, imgR,min_disp, max_disp, disp_size, window_radius):\n",
    "    num_disp = max_disp-min_disp\n",
    "    h, w = imgL.shape\n",
    "    \n",
    "    #The cropped center area we are interested in\n",
    "    croph = [(h-disp_size)//2,(h+disp_size)//2]\n",
    "    cropw = [(w-disp_size)//2,(w+disp_size)//2]\n",
    "\n",
    "    ref_img_cropped = imgL[croph[0]:croph[1],cropw[0]:cropw[1]]\n",
    "\n",
    "    #Simple sum kernel for our filter\n",
    "    kernel = np.ones([window_radius, window_radius])\n",
    "\n",
    "    #C(y,x,d) is the 3D volume containing the cost of a disparity at pixel x,y in the reference\n",
    "    C = np.zeros(\n",
    "        [disp_size, disp_size, max_disp])\n",
    "\n",
    "    for d in range(min_disp, max_disp):\n",
    "        # Shift image d pixels horizontally\n",
    "        translation_matrix = np.float32([[1, 0, d], [0, 1, 0]])\n",
    "        shifted_image = cv2.warpAffine(\n",
    "            imgR, translation_matrix,\n",
    "            (imgR.shape[1], imgR.shape[0]))\n",
    "        shifted_image_cropped = shifted_image[croph[0]:croph[1],cropw[0]:cropw[1]]\n",
    "        # Absolute difference\n",
    "        AD = abs(np.float32(ref_img_cropped) - np.float32(shifted_image_cropped))\n",
    "        # Calculate SAD.\n",
    "        C[:, :, d] = cv2.filter2D(AD, -1, kernel)\n",
    "    #Add min_disp as the index is shifted\n",
    "    disparity = min_disp + np.argmin(C[:,:,min_disp:max_disp], axis=2)\n",
    "    return disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two simple functions to calculate the main disparity. Using the most frequent value seems to be the best of these two. We want to filter out outliers, as there are some completely wrong disparities genereted. Even with a ground truth disparity map, we need to decide the main disparity. The camera might see multiple obstacles at different distances. If we for example mostly see the checkerboard, but also some part of the background, the average method will get the distance somewhere in between. The most frequent method would get the distance to the checkerboard assuming it is the most frequent. \n",
    "\n",
    "Another porposal: Using the histogram, we arrange the disparities in bins, and choose the closest in distance bin with a sufficient amount of disparities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_disparity_mean(disparity):\n",
    "    return np.mean(disparity)\n",
    "def main_disparity_most_frequent(disparity):\n",
    "    return np.bincount(disparity.flatten()).argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a distance from the main disparity, we need some information about the cameras and their position. We assume the cameras to be equal with a focal length f, and the stereo rig to only have a horizontal displacement between the cameras, the baseline b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_from_disparity(main_disp, f, b):\n",
    "    distance = (f*b)/(main_disp*1000) #meter\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main disparity should lie within a smaller range in each time instant than the range for the wholeÂ video. We can exploit this by dynamically changing the range, assuming that the main disparity does not change by a large amount frame to frame. This has a huge performance advantage, as we can reduce the number of disparities that needs to calculated and compared.\n",
    "\n",
    "Non dynamic disparity calculation was tested on a laptop, running at around 7 fps with min_disp = 0, max_disp = 128, disparity_area_size = 100 and window_size = 8.\n",
    "With dynamic disparity calculation, the fps was around 30 to 40, a huge increase. max_disp was set to 32, the rest of the settings the same. Distance calculation had no noticable difference between the runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_disparity_matrix_dynamic_range(imgL, imgR,min_disp, max_disp, disp_size, window_radius, main_disp):\n",
    "    num_disp = max_disp-min_disp\n",
    "    h, w = imgL.shape\n",
    "    \n",
    "    #Use the previous main_disp to offset disparity search around the main disparity\n",
    "    disp_offset = int(main_disp - max_disp//2)\n",
    "    if(disp_offset>0):\n",
    "        max_disp += disp_offset\n",
    "        min_disp += disp_offset\n",
    "\n",
    "    #The cropped center area we are interested in\n",
    "    croph = [(h-disp_size)//2,(h+disp_size)//2]\n",
    "    cropw = [(w-disp_size)//2,(w+disp_size)//2]\n",
    "\n",
    "    ref_img_cropped = imgL[croph[0]:croph[1],cropw[0]:cropw[1]]\n",
    "\n",
    "    #Simple sum kernel for our filter\n",
    "    kernel = np.ones([window_radius, window_radius])\n",
    "\n",
    "    #C(y,x,d) is the 3D volume containing the cost of a disparity at pixel x,y in the reference\n",
    "    C = np.zeros(\n",
    "        [disp_size, disp_size, max_disp])\n",
    "\n",
    "    for d in range(min_disp, max_disp):\n",
    "        # Shift image d pixels horizontally\n",
    "        translation_matrix = np.float32([[1, 0, d], [0, 1, 0]])\n",
    "        shifted_image = cv2.warpAffine(\n",
    "            imgR, translation_matrix,\n",
    "            (imgR.shape[1], imgR.shape[0]))\n",
    "        shifted_image_cropped = shifted_image[croph[0]:croph[1],cropw[0]:cropw[1]]\n",
    "        # Absolute difference\n",
    "        AD = abs(np.float32(ref_img_cropped) - np.float32(shifted_image_cropped))\n",
    "        # Calculate SAD.\n",
    "        C[:, :, d] = cv2.filter2D(AD, -1, kernel)\n",
    "    #Add min_disp as the index is shifted\n",
    "    disparity = min_disp + np.argmin(C[:,:,min_disp:max_disp], axis=2)\n",
    "    return disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the chessboard, we use the OpenCV function findChessboardCorners, which will return the corners of the internal squares. If we want, we can get some better accuracy by using a subpixel corner refinement algorith,, at the expense of computation time. \n",
    "\n",
    "To find the dimensions, we use the same formula as used to calculate depth from disparity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_chessboard(img, pattern_size):\n",
    "    # Finding corners\n",
    "    found, corners = cv2.findChessboardCorners(img, pattern_size)\n",
    "    if found:\n",
    "        # Refining corner position. Remove if performance is bad\n",
    "        term = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_COUNT, 5, 1)\n",
    "        cv2.cornerSubPix(img, corners, (5, 5), (-1, -1), term)\n",
    "        return True, corners\n",
    "    else:\n",
    "        return False, None\n",
    "        \n",
    "def calculate_chessboard_dimensions(distance, corners, pattern_size, f):\n",
    "    #Calculate pixel difference between top left and top right corner, and top left and bottom left corner respectively\n",
    "    w_pixel = abs(corners[0,0,0]-corners[pattern_size[0]-1,0,0])\n",
    "    h_pixel = abs(corners[0,0,1]-corners[(pattern_size[0]-1)*pattern_size[1]-1,0,1])    \n",
    "    #Calculate dimensions in mm \n",
    "    w = (distance*w_pixel)/f #mm\n",
    "    h = (distance*h_pixel)/f #mm\n",
    "    return w,h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters that are not given are set by experimentation. The visual representation of the disparity map was used for tuning. For example the window size was determined looking at the disparity map with minimal cropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "#Stereo cameras\n",
    "focal_length = 567.2 #pixels\n",
    "baseline = 92.226 #mm\n",
    "\n",
    "#Stereo matching algorithm\n",
    "min_disp=0\n",
    "max_disp=32\n",
    "\n",
    "num_disp = max_disp-min_disp\n",
    "disparity_area_size = 60\n",
    "window_radius = 8\n",
    "\n",
    "#Chessboard\n",
    "pattern_size = (6,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a GUI with tkinter. Here we display the rgb camera feed from the left camera, the disparity map transformed to a color heat map, the distance to an obstacle, as well as the fps. Optionally the chessboard dimensions will be calculated and displayed. The video will run as fast as calculations are done. If we had a real time camera feed, this could be pretty easily altered to do the next calculation on the newest frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWindow():\n",
    "    def __init__(self, window):\n",
    "        self.use_dynamic_disparity_range = True\n",
    "        self.calculate_checkerboard = False\n",
    "        self.main_disp = max_disp//2\n",
    "\n",
    "        self.window = window\n",
    "        self.vidL = cv2.VideoCapture('../data/robotL.avi')\n",
    "        self.vidR = cv2.VideoCapture('../data/robotR.avi')\n",
    "        if (self.vidL.isOpened()==False or self.vidR.isOpened()==False):\n",
    "            print(\"Error opening video\")\n",
    "\n",
    "        self.width = 800\n",
    "        self.height = 600\n",
    "        self.interval = 1 # Interval in ms to get the latest frame\n",
    "\n",
    "        self.distance_widget = tk.Label(root, text=\"Distance: 0\", width=100)\n",
    "        self.distance_widget.config(font=(\"Courier\", 20))\n",
    "        self.dimension_widget = tk.Label(root, text=\"Chessboard dimensions: \", width=100)\n",
    "        self.dimension_widget.config(font=(\"Courier\", 20))\n",
    "        self.fps_widget = tk.Label(root, text=\"FPS: 0\", width=100)\n",
    "        self.fps_widget.config(font=(\"Courier\", 20))\n",
    "\n",
    "        # Create canvas for gui\n",
    "        self.canvas = tk.Canvas(self.window, width=self.width, height=self.height)\n",
    "        self.canvas.grid(row=0, column=0)\n",
    "        # Update gui\n",
    "        self.update_gui()\n",
    "    def update_gui(self):\n",
    "\n",
    "        # Capture frame-by-frame\n",
    "        retL, frameL = self.vidL.read()\n",
    "        retR, frameR = self.vidR.read()\n",
    "        if retL == True and retR == True:\n",
    "            frameL_greyscale=cv2.cvtColor(frameL, cv2.COLOR_BGR2GRAY)\n",
    "            frameR_greyscale=cv2.cvtColor(frameR, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "        \n",
    "            start = time.time()\n",
    "            \n",
    "            if(self.use_dynamic_disparity_range):\n",
    "                disparity = calculate_disparity_matrix_dynamic_range(frameL_greyscale, frameR_greyscale,min_disp, max_disp, \n",
    "                disparity_area_size, window_radius, self.main_disp)\n",
    "            else:\n",
    "                disparity = calculate_disparity_matrix(frameL_greyscale, frameR_greyscale,min_disp, max_disp, \n",
    "                disparity_area_size, window_radius)\n",
    "            \n",
    "            self.main_disp = main_disparity_most_frequent(disparity)\n",
    "            \n",
    "            distance = get_distance_from_disparity(self.main_disp, focal_length, baseline)\n",
    "            \n",
    "            \n",
    "            #Find chessboard corners\n",
    "            if self.calculate_checkerboard:\n",
    "                ret, corners = find_chessboard(frameL_greyscale, pattern_size)\n",
    "                if ret:\n",
    "                    #frameL = cv2.resize(frameL, (frameL.shape[1]//8, frameL.shape[0]//8))\n",
    "                    cv2.drawChessboardCorners(frameL, pattern_size, corners, True)\n",
    "                    w,h = calculate_chessboard_dimensions(distance*1000, corners, pattern_size, focal_length)\n",
    "                    error_w, error_h = abs(125 - w), abs(178-h)\n",
    "                    self.dimension_widget[\"text\"] = (\"Chessboard dimensions: \"+str(round(w)) + \"mm x \" \n",
    "                        + str(round(h)) + \"mm. Error: \" + str(round(error_w)) + \"mm , \" + str(round(error_h)) + \"mm\")\n",
    "                    self.dimension_widget.config(fg=\"green\")\n",
    "                else:\n",
    "                    self.dimension_widget.config(fg=\"red\")\n",
    "            \n",
    "            #We calculate fps without GUI, as that is not as interesting\n",
    "            fps = 1/(time.time()-start)\n",
    "            # Scale disparity values for visualization \n",
    "            disparity_visual = np.uint8(disparity * 128 / num_disp)\n",
    "            disp_heatmap = cv2.applyColorMap(disparity_visual, cv2.COLORMAP_JET)\n",
    "    \n",
    "            blue,green,red = cv2.split(frameL)\n",
    "            self.image_rgb = cv2.merge((red,green,blue))\n",
    "            self.image_rgb = Image.fromarray(self.image_rgb)\n",
    "            self.image_rgb = ImageTk.PhotoImage(image=self.image_rgb, master=root)\n",
    "\n",
    "            # Update image\n",
    "            self.canvas.create_image(0, 0, anchor=tk.NW, image=self.image_rgb)\n",
    "\n",
    "            blue,green,red = cv2.split(disp_heatmap)\n",
    "            self.image_disp = cv2.merge((red,green,blue))\n",
    "            self.image_disp = Image.fromarray(self.image_disp)\n",
    "            self.image_disp = ImageTk.PhotoImage(image=self.image_disp, master=root)\n",
    "            \n",
    "            self.canvas.create_image(680, 250, anchor=tk.NW, image=self.image_disp)\n",
    "            if(distance<0.8):\n",
    "                self.distance_widget[\"text\"] = \"ALARM! Distance to obstacle: \"+str(round(distance,2)) + \"m\"\n",
    "                self.distance_widget.config(fg=\"red\")\n",
    "            else:\n",
    "                self.distance_widget[\"text\"] = \"Distance to obstacle: \"+str(round(distance,2)) + \"m\"\n",
    "                self.distance_widget.config(fg=\"black\")\n",
    "\n",
    "            self.fps_widget[\"text\"] = \"FPS: \" + str(round(fps))\n",
    "\n",
    "            self.distance_widget.grid()\n",
    "            self.dimension_widget.grid()\n",
    "            self.fps_widget.grid()\n",
    "\n",
    "            self.window.after(self.interval, self.update_gui)\n",
    "            \n",
    "\n",
    "        # Break the loop\n",
    "        else:             \n",
    "            # When everything done, release the video capture object\n",
    "            self.vidL.release()\n",
    "            self.vidR.release()\n",
    "            # Closes all the frames\n",
    "            cv2.destroyAllWindows()\n",
    "            quit()\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    MainWindow(root)\n",
    "    root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
